{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PARAPHRASING AUGMENTATION PIPELINE (Model-Agnostic)\n",
    "# ============================================================================\n",
    "\n",
    "# STEP 1: Load Input Data\n",
    "# ----------------------------------------------------------------------------\n",
    "Input: Alpaca-format JSON file ('instruction' key were cleaned first for simplicity)\n",
    "  - Each item: { \"id\": str, \"input\": str, \"output\": str, ... }\n",
    "  - output field contains JSON: {\"<CATEGORY_NAME>\": [\"<SPAN_1>\", \"<SPAN_2>\", ...]}\n",
    "\n",
    "Load alpaca_data = json.load(input_file)\n",
    "\n",
    "\n",
    "# STEP 2: Extract Context Window for Each Span\n",
    "# ----------------------------------------------------------------------------\n",
    "For each item in alpaca_data:\n",
    "    original_text = item[\"input\"]\n",
    "    output_data = json.loads(item[\"output\"])\n",
    "    \n",
    "    For each category_key, span_list in output_data.items():\n",
    "        For each span in span_list:\n",
    "            # Find span location in text (case-insensitive)\n",
    "            span_match = find_span_in_text(original_text, span)\n",
    "            \n",
    "            # Extract context window: N words before + span + N words after\n",
    "            context_window = extract_context_window(\n",
    "                text=original_text,\n",
    "                span=span,\n",
    "                window_size=WINDOW_SIZE  # e.g., 20 words, if less than 20 words, then take all words available\n",
    "            )\n",
    "            # Result: \"<CONTEXT_BEFORE> <SPAN> <CONTEXT_AFTER>\"\n",
    "\n",
    "\n",
    "# STEP 3: Build Paraphrasing Prompt\n",
    "# ----------------------------------------------------------------------------\n",
    "System Prompt Template:\n",
    "\"\"\"\n",
    "You are assisting in controlled paraphrasing of de-identified clinical text. \n",
    "Your goal is to rephrase the surrounding context of a verified indicator span \n",
    "while keeping the indicator phrase itself verbatim and preserving the clinical meaning of the entire segment (span + surrounding context). \n",
    "Do not invent new clinical information or identifiers.\n",
    "\"\"\"\n",
    "# ----------------------------------------------------------------------------\n",
    "User Prompt Template:\n",
    "\"\"\"\n",
    "Below is a clinical note and a specific segment extracted from it that needs paraphrasing.\n",
    "\n",
    "FULL NOTE (for context reference):\n",
    "<FULL_NOTE_TEXT>\n",
    "TARGET SEGMENT:\n",
    "Original Context Window:\n",
    "<CONTEXT_BEFORE> <SPAN> <CONTEXT_AFTER>\n",
    "Target Span to Keep Unchanged: \"<SPAN>\"\n",
    "\n",
    "Use the full note to maintain clinical coherence, but paraphrase only the surrounding context (before and after the span) in the target segment above. Keep the target span verbatim.\n",
    "Now generate <AUG_INSTANCE_#> paraphrased versions of the surrounding context. Follow the same output format.\n",
    "Rewritten Context Window:\n",
    "\"\"\"\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "prompt = build_prompt(context_window, span)\n",
    "\n",
    "\n",
    "# STEP 4: Model Interface\n",
    "# ----------------------------------------------------------------------------\n",
    "For each span, generate num_augmentations_per_span versions:\n",
    "    For aug_num in range(num_augmentations_per_span):\n",
    "        # Call model\n",
    "        model_response = call_model(\n",
    "            prompt=prompt,\n",
    "            model_config={\n",
    "                \"temperature\": 0.4,\n",
    "                \"top_k\": 32,\n",
    "                \"max_tokens\": 1024\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        paraphrased_context = extract_text_from_response(model_response)\n",
    "\n",
    "\n",
    "# STEP 5: Validate & Process Model Output\n",
    "# ----------------------------------------------------------------------------\n",
    "# Validation checks:\n",
    "if span not in paraphrased_context:\n",
    "    skip this augmentation\n",
    "    \n",
    "# Replace original context window in full text\n",
    "augmented_text = original_text.replace(\n",
    "    old=original_context_window,\n",
    "    new=paraphrased_context,\n",
    "    count=1  # Replace first occurrence only\n",
    ")\n",
    "\n",
    "# Verify change occurred\n",
    "if augmented_text == original_text:\n",
    "    skip this augmentation\n",
    "\n",
    "\n",
    "# STEP 6: Create Augmented Item\n",
    "# ----------------------------------------------------------------------------\n",
    "new_item = {\n",
    "    \"input\": augmented_text,  # Full note with paraphrased context\n",
    "    \"output\": item[\"output\"],  # Original output unchanged\n",
    "    \"id\": f\"{base_id}_aug_s{span_index}_a{aug_num+1}\",\n",
    "    \"base_item_id\": base_id,\n",
    "    # ... other fields from original item\n",
    "}\n",
    "\n",
    "\n",
    "# STEP 7: Output Format\n",
    "# ----------------------------------------------------------------------------\n",
    "Output: JSON file containing only augmented items\n",
    "  - Each augmented item is a complete Alpaca-format instance\n",
    "  - Original items are NOT included in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SYNTHETIC GENERATION PIPELINE (Two-Round Cascaded Workflow)\n",
    "# ============================================================================\n",
    "\n",
    "# STEP 1: Load Input Data\n",
    "# ----------------------------------------------------------------------------\n",
    "Input: Stage 2 corpus (Alpaca-format JSON file)\n",
    "  - Each item: { \"id\": str, \"input\": str, \"output\": str, ... }\n",
    "  - output field contains JSON: {\"<CATEGORY_NAME>\": [\"<SPAN_1>\", \"<SPAN_2>\", ...]}\n",
    "\n",
    "Load stage2_data = json.load(input_file)\n",
    "\n",
    "# Group instances by category\n",
    "category_instances = group_by_category(stage2_data)\n",
    "# Result: { \"<CATEGORY_NAME>\": [item1, item2, ...], ... }\n",
    "\n",
    "\n",
    "# STEP 2: Round 1 - Generation\n",
    "# ----------------------------------------------------------------------------\n",
    "For each category in category_instances:\n",
    "    category_name = category\n",
    "    category_def = load_category_definition(category_name)  # From Appendix 4-Table 2\n",
    "    \n",
    "    For each request in Round 1:\n",
    "        # Randomly select 3 demonstrations from Stage 2 corpus for this category\n",
    "        demonstrations = random.sample(\n",
    "            category_instances[category_name], \n",
    "            k=3\n",
    "        )\n",
    "        \n",
    "        # Build prompt with demonstrations\n",
    "        prompt = build_synthetic_prompt(\n",
    "            category_name=category_name,\n",
    "            category_definition=category_def,\n",
    "            demonstrations=demonstrations,\n",
    "            num_instances=K  # Generate K candidates per request\n",
    "        )\n",
    "        \n",
    "        # Call model\n",
    "        model_response = call_model(prompt=prompt)\n",
    "        \n",
    "        # Parse response to extract synthetic instances\n",
    "        synthetic_candidates_R1 = parse_synthetic_response(model_response)\n",
    "        # Result: List of { \"input\": str, \"output\": str } pairs\n",
    "\n",
    "\n",
    "# STEP 3: Round 1 - Validation\n",
    "# ----------------------------------------------------------------------------\n",
    "For each candidate in synthetic_candidates_R1:\n",
    "    # Apply validation checks (if any)\n",
    "    if passes_validation(candidate):\n",
    "        validation_queue_R1.append(candidate)\n",
    "\n",
    "# Manual Validation (by 2 HIPAA-trained reviewers)\n",
    "For each candidate in validation_queue_R1:\n",
    "    reviewer1_decision = manual_validate(candidate)\n",
    "    reviewer2_decision = manual_validate(candidate)\n",
    "    \n",
    "    if reviewer1_decision == \"pass\" and reviewer2_decision == \"pass\":\n",
    "        verified_seeds_R1.append(candidate)\n",
    "        candidate[\"id\"] = f\"{category_name}_syn_R1_{len(verified_seeds_R1)}\"\n",
    "        candidate[\"round\"] = 1\n",
    "        candidate[\"category\"] = category_name\n",
    "\n",
    "\n",
    "# STEP 4: Round 2 - Expansion\n",
    "# ----------------------------------------------------------------------------\n",
    "For each category:\n",
    "    # Use verified seeds from Round 1 as demonstrations\n",
    "    seeds_for_round2 = verified_seeds_R1[category]\n",
    "    \n",
    "    For each request in Round 2:\n",
    "        # Select seed(s) for this request (use verified seeds as demonstrations)\n",
    "        selected_seeds = random.sample(seeds_for_round2, k=min(3, len(seeds_for_round2)))\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = build_synthetic_prompt(\n",
    "            category_name=category_name,\n",
    "            category_definition=category_def,\n",
    "            demonstrations=selected_seeds,\n",
    "            num_instances=K\n",
    "        )\n",
    "        \n",
    "        # Call model\n",
    "        model_response = call_model(prompt=prompt)\n",
    "        \n",
    "        # Parse response to extract synthetic instances\n",
    "        synthetic_candidates_R2 = parse_synthetic_response(model_response)\n",
    "\n",
    "\n",
    "# STEP 5: Round 2 - Validation\n",
    "# ----------------------------------------------------------------------------\n",
    "For each candidate in synthetic_candidates_R2:\n",
    "    # Apply validation checks (if any)\n",
    "    if passes_validation(candidate):\n",
    "        validation_queue_R2.append(candidate)\n",
    "\n",
    "# Manual Validation (by 2 HIPAA-trained reviewers)\n",
    "For each candidate in validation_queue_R2:\n",
    "    reviewer1_decision = manual_validate(candidate)\n",
    "    reviewer2_decision = manual_validate(candidate)\n",
    "    \n",
    "    if reviewer1_decision == \"pass\" and reviewer2_decision == \"pass\":\n",
    "        verified_final_R2.append(candidate)\n",
    "        candidate[\"id\"] = f\"{category_name}_syn_R2_{len(verified_final_R2)}\"\n",
    "        candidate[\"round\"] = 2\n",
    "        candidate[\"category\"] = category_name\n",
    "\n",
    "\n",
    "# STEP 6: Output Format\n",
    "# ----------------------------------------------------------------------------\n",
    "# Combine all verified synthetic instances from both rounds\n",
    "final_synthetic_data = verified_seeds_R1 + verified_final_R2\n",
    "\n",
    "# Output: JSON file containing only synthetic items\n",
    "#   - Each synthetic item is a complete Alpaca-format instance\n",
    "#   - Only manually validated instances are included\n",
    "#   - Original Stage 2 items are NOT included in this output\n",
    "\n",
    "save_to_json(final_synthetic_data, output_file=\"stage3_synthetic_data.json\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT TEMPLATE FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def build_synthetic_prompt(category_name, category_definition, demonstrations, num_instances):\n",
    "    \"\"\"\n",
    "    Build prompt following Table 3 format.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    You are generating de-identified, clinically plausible home-health-care (HHC) instances, \n",
    "    each consisting of a note and its corresponding structured output span. Follow the rules below: \n",
    "    \n",
    "    1. Write a HHC note as if authored by a clinician. Learn from the style of the demonstrations \n",
    "       provided and allow occasional typing or syntax errors typical of real notes. \n",
    "    2. Maintain accurate clinical meaning and realistic context typical of home settings. \n",
    "    3. Keep all content absent of personal identifiers. \n",
    "    4. Include the indicator text span verbatim within the note. \n",
    "    5. Output the JSON extraction exactly as: {\"<CATEGORY_NAME>\": [\"<indicator text span>\"]} \n",
    "    6. Do not repeat prior instances; create new variations consistent with the definition of the category.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    TARGET CATEGORY: {category_name}\n",
    "    \n",
    "    TASK OVERVIEW:\n",
    "    1. Generate Input: Create a plausible clinical note snippet (input) that realistically \n",
    "       incorporates information relevant to the TARGET CATEGORY: {category_name}.\n",
    "    2. Generate Output: Based only on the input snippet you just generated, create the \n",
    "       corresponding output JSON string. This JSON maps the category name \"{category_name}\" \n",
    "       to a list of verbatim text spans extracted directly from your generated input. \n",
    "    \n",
    "    KEY GUIDELINES FOR {category_name}:\n",
    "    {category_definition}\n",
    "    \n",
    "    Below are validated real or paraphrased instances illustrating how this category appears \n",
    "    in home-health-care notes. Use these demonstrations to understand tone, structure, and \n",
    "    JSON output format. Strictly follow the Input/Output format demonstrated below.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add demonstrations\n",
    "    for i, demo in enumerate(demonstrations, 1):\n",
    "        user_prompt += f\"\"\"\n",
    "    Example {i}\n",
    "    Input: \"{demo['input']}\"\n",
    "    Output: {demo['output']}\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt += f\"\"\"\n",
    "    Now generate {num_instances} entirely new synthetic instances following the same format. \n",
    "    \n",
    "    GENERATE THE NEXT EXAMPLE:\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"system\": system_prompt,\n",
    "        \"user\": user_prompt\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
